{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05076e1f-2db5-4cfe-a01b-af86e3b0ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd   \n",
    "import numpy as np    \n",
    "import random         \n",
    "\n",
    "# Set pandas option to display a large number of rows (6000 in this case)\n",
    "# pd.set_option('display.max_rows', None)  # Option to display all rows, commented out\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings for cleaner output\n",
    "pd.set_option('display.max_rows', 6000)  # Set the max number of rows to display to 6000\n",
    "\n",
    "# Importing datetime for date manipulation\n",
    "from datetime import timedelta\n",
    "\n",
    "# Importing machine learning utilities from scikit-learn\n",
    "from sklearn.utils import resample  # For resampling the dataset\n",
    "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
    "from sklearn.ensemble import RandomForestClassifier  # For the Random Forest model\n",
    "from sklearn import metrics, preprocessing  # For evaluation metrics and preprocessing\n",
    "from sklearn.model_selection import cross_val_score  # For cross-validation\n",
    "from sklearn.metrics import classification_report  # For generating classification reports\n",
    "\n",
    "# Importing tqdm for progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a9fcdd-2471-4a0d-bcb0-716a21f61a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pickle for object serialization\n",
    "import pickle  # For saving and loading model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50a7807-3d9e-4a69-ab6e-7a0033302902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from a pickle file\n",
    "with open('2023-07-04.pkl', 'rb') as file:  # Open the pickle file in read-binary mode\n",
    "    data = pickle.load(file)  # Load the data from the pickle file into the variable 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627284f9-4d84-4f25-9153-9faea3262544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking the data loaded from the pickle file\n",
    "keystrokes, accs, gyrs = data  # Assuming 'data' is a tuple or list with three elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8562e6-1c2f-4b2a-ba94-01681c3f1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting a random sample of indices for testing\n",
    "test_sess_index = random.sample(range(len(keystrokes)), round(0.2 * len(keystrokes)))\n",
    "\n",
    "# 'len(keystrokes)' gives the total number of sessions or data points\n",
    "# 'round(0.2 * len(keystrokes))' calculates 20% of the total sessions and rounds to the nearest integer\n",
    "# 'random.sample(range(len(keystrokes)), round(0.2 * len(keystrokes)))' selects random indices from the range\n",
    "# of session indices, equivalent to choosing 20% of the sessions randomly for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6aa2a0-7dcc-4298-b438-8a6ff935800f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 15, 1, 12, 24, 33, 26, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of randomly selected indices\n",
    "test_sess_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc1a3a3-9338-4b5a-ba4a-8641c664dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ids(x):\n",
    "    final = pd.DataFrame()  # Initialize an empty DataFrame to store final data\n",
    "    for i in range(len(keystrokes)):  # Iterate through all sessions or data points\n",
    "        if i not in test_sess_index:  # Check if the session index 'i' is not in the test set\n",
    "            x[i]['id'] = i  # Assign 'id' column with the session index 'i' to the dataframe x[i]\n",
    "            final = pd.concat([final, x[i]])  # Concatenate dataframe x[i] to the final DataFrame\n",
    "    return final  # Return the final concatenated DataFrame\n",
    "\n",
    "def create_test_ids(x):\n",
    "    final = pd.DataFrame()  # Initialize an empty DataFrame to store final test data\n",
    "    for i in test_sess_index:  # Iterate through the indices selected for testing\n",
    "        x[i]['id'] = i  # Assign 'id' column with the session index 'i' to the dataframe x[i]\n",
    "        final = pd.concat([final, x[i]])  # Concatenate dataframe x[i] to the final test DataFrame\n",
    "    return final  # Return the final concatenated test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf8ad90-0cb2-45c4-b3af-483e8c398fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets with 'id' columns for training\n",
    "key = create_ids(keystrokes)  # Create 'key' dataset with 'id' column from 'keystrokes'\n",
    "acc = create_ids(accs)        # Create 'acc' dataset with 'id' column from 'accs'\n",
    "gyr = create_ids(gyrs)        # Create 'gyr' dataset with 'id' column from 'gyrs'\n",
    "\n",
    "# Creating datasets with 'id' columns for testing\n",
    "t_key = create_test_ids(keystrokes)  # Create 't_key' dataset with 'id' column from 'keystrokes' for testing\n",
    "t_acc = create_test_ids(accs)        # Create 't_acc' dataset with 'id' column from 'accs' for testing\n",
    "t_gyr = create_test_ids(gyrs)        # Create 't_gyr' dataset with 'id' column from 'gyrs' for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54587b8c-3b02-48db-8a31-50ee1d778e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.reset_index(drop=True, inplace=True)  # Resetting the index of 'acc' DataFrame\n",
    "gyr.reset_index(drop=True, inplace=True)  # Resetting the index of 'gyr' DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045a2a41-b590-4f49-8e01-17a0c9346410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22362/22362 [03:10<00:00, 117.43it/s]\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame()  # Initialize an empty DataFrame to store results\n",
    "matched_rows = {}  # Initialize an empty dictionary to track matched rows\n",
    "\n",
    "# Renaming columns in 'gyr' DataFrame\n",
    "gyr = gyr.rename({'x': 'x_g', 'y': 'y_g', 'z': 'z_g'}, axis=1)\n",
    "\n",
    "# Iterate over rows in 'gyr' DataFrame using tqdm for progress tracking\n",
    "for index, row in tqdm(gyr.iterrows(), total=gyr.shape[0]):\n",
    "    # Calculate the absolute time difference between 'acc' and current 'gyr' row\n",
    "    time_diffs = abs(acc['t'] - row['t'])\n",
    "    min_time_diff = time_diffs.min()  # Find the minimum time difference\n",
    "    \n",
    "    closest_index = time_diffs.idxmin()  # Find the index of the minimum time difference\n",
    "    matched_rows[closest_index] = True  # Mark the index as matched in the dictionary\n",
    "\n",
    "    # Rename and convert the current 'gyr' row to a DataFrame row\n",
    "    row_adjusted = row.rename({'t': 't2', 'id': 'id2'}).to_frame().T.reset_index(drop=True)\n",
    "\n",
    "    # Combine the closest 'acc' row and the adjusted 'gyr' row into a single DataFrame row\n",
    "    combined_row = pd.concat([acc.iloc[[closest_index]].reset_index(drop=True), row_adjusted], axis=1)\n",
    "    combined_row['TimeDiff'] = min_time_diff  # Add the 'TimeDiff' column to the combined row\n",
    "\n",
    "    # Concatenate the combined row to the 'result_df' DataFrame\n",
    "    result_df = pd.concat([result_df, combined_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a33dfc-e0b8-4031-b81e-76e086b4a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.set_index('t')  # Setting 't' column as index in 'acc' DataFrame (but not assigning the result to acc)\n",
    "result_df.set_index('t')  # Setting 't' column as index in 'result_df' DataFrame (but not assigning the result to result_df)\n",
    "\n",
    "# Merging 'acc' and 'result_df' DataFrames based on 't' and 'id' columns, using outer join\n",
    "merged_df = pd.merge(acc, result_df, on=['t', 'id'], how='outer').sort_values(['id', 't'])\n",
    "\n",
    "# Dropping unnecessary columns from merged DataFrame\n",
    "merged_df.drop(['x_y', 'y_y', 'z_y', 'id2'], axis=1, inplace=True)\n",
    "\n",
    "# Renaming columns in merged DataFrame\n",
    "merged_df.rename({'x_x': 'x', 'y_x': 'y', 'z_x': 'z'}, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14efd60-851a-435f-ad45-787b6914d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = merged_df.copy()\n",
    "\n",
    "# This line creates a deep copy of the DataFrame 'merged_df' and assigns it to 'merged_df2'.\n",
    "# Changes made to 'merged_df2' will not affect 'merged_df', and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dadc58e-234a-452b-bdaa-f67c3caedd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'x_g' column to numeric values, coercing errors to NaN\n",
    "merged_df2['x_g'] = pd.to_numeric(merged_df2['x_g'], errors='coerce')\n",
    "\n",
    "# Convert 'y_g' column to numeric values, coercing errors to NaN\n",
    "merged_df2['y_g'] = pd.to_numeric(merged_df2['y_g'], errors='coerce')\n",
    "\n",
    "# Convert 'z_g' column to numeric values, coercing errors to NaN\n",
    "merged_df2['z_g'] = pd.to_numeric(merged_df2['z_g'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e90c4822-1931-4a70-9729-42b1bc0815d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df2[['x_g', 'y_g', 'z_g']] = merged_df2.groupby('id')[['x_g', 'y_g', 'z_g']].apply(lambda group: group.interpolate(method='polynomial',order=5))\n",
    "merged_df2[['x_g', 'y_g', 'z_g']] = merged_df2.groupby('id')[['x_g', 'y_g', 'z_g']].apply(lambda group: group.interpolate(method='linear'))\n",
    "#merged_df2[['x_g', 'y_g', 'z_g']] = merged_df2.groupby('id')[['x_g', 'y_g', 'z_g']].apply(lambda group: group.interpolate(method='spline',order=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "729c5eab-c9ca-4221-81c1-2a54d30f2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = merged_df2.isnull().sum()\n",
    "\n",
    "# This line calculates the number of missing values (NaN) in each column of the DataFrame 'merged_df2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6929878f-f72f-4516-818b-2875a0315e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'key' is a DataFrame\n",
    "\n",
    "# Drop the 'ut' column from 'key'\n",
    "# key.drop('ut', axis=1, inplace=True)\n",
    "\n",
    "# Add a new column 'target' with all values set to 1 in 'key'\n",
    "key['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0306a8b2-5c44-4fb2-9771-1ef3af8c03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column 'dt' to 't' in the DataFrame 'key'\n",
    "key = key.rename({'dt': 't'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb17cbd5-a50a-42be-abe9-104c338ee39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nans(df):\n",
    "    # Sort the DataFrame 'df' by the column 't'\n",
    "    df = df.sort_values(by='t')\n",
    "    \n",
    "    # Find the times where 'target' is 1.0\n",
    "    times_with_1 = df[df['target'] == 1.0]['t']\n",
    "\n",
    "    # Iterate over these times and replace NaNs within 0.3 seconds before and after\n",
    "    for time in times_with_1:\n",
    "        start_time = time - timedelta(seconds=0.6)\n",
    "        end_time = time + timedelta(seconds=0.6)\n",
    "        \n",
    "        # Locate rows where 't' is within the specified time range and 'target' is NaN\n",
    "        df.loc[(df['t'] >= start_time) & (df['t'] <= end_time) & (df['target'].isna()), 'target'] = 1.0\n",
    "\n",
    "    # Replace remaining NaNs in 'target' with 0\n",
    "    df['target'].fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed66756b-58c5-4fec-a709-c3093d66c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(acce, gyro, keys):\n",
    "    # Combining acc and gyr\n",
    "    keys = keys.rename({'dt':'t'},axis=1)\n",
    "    gyro = gyro.rename({'x': 'x_g', 'y': 'y_g', 'z': 'z_g'}, axis=1)\n",
    "    df = pd.concat([acce, gyro]).sort_values(['id','t'])\n",
    "    df[['x_g', 'y_g', 'z_g']] = df.groupby('id')[['x_g', 'y_g', 'z_g']].apply(lambda group: group.interpolate(method='linear'))\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    # Removing 'ut' and creating the target value\n",
    "    keys.drop('ut', axis=1, inplace=True)\n",
    "    keys['target'] = 1\n",
    "\n",
    "    # Combining acc, gyr, and keys\n",
    "    merged_df = pd.merge(df, keys, on=['t','id'], how='outer').sort_values('t')\n",
    "\n",
    "    labeled_df = replace_nans(merged_df)\n",
    "\n",
    "    # Convert 'target' column back to numeric\n",
    "    labeled_df['target'] = pd.to_numeric(labeled_df['target'])\n",
    "\n",
    "    labeled_df.dropna(inplace=True)\n",
    "\n",
    "    return labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb9a517d-1d93-496c-a4e2-98ca38b1e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming acc is the DataFrame with accelerometer data\n",
    "res_acc = acc  # Placeholder, replace with actual processing code\n",
    "\n",
    "# Assuming gyr is the DataFrame with gyroscope data\n",
    "res_gyr = gyr  # Placeholder, replace with actual processing code\n",
    "\n",
    "res_acc_t = t_acc  # Placeholder, replace with actual test accelerometer data processing\n",
    "res_gyr_t = t_gyr  # Placeholder, replace with actual test gyroscope data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5daac81e-d57e-4f7f-b0b9-124d5ba50c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sessions = preprocessing(res_acc,res_gyr,key)\n",
    "test_sessions = preprocessing(res_acc_t,res_gyr_t,t_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a9865ee-15e7-453c-bbac-382e0e2bfb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fft_features(df, column, window_size):\n",
    "    \n",
    "    # Function to calculate energy of FFT\n",
    "    def fft_energy(x):\n",
    "        return np.sum(np.abs(np.fft.fft(x))**2) / len(x)\n",
    "\n",
    "    # Function to calculate entropy of FFT\n",
    "    def fft_entropy(x):\n",
    "        fft_abs = np.abs(np.fft.fft(x))\n",
    "        return -np.sum((fft_abs / np.sum(fft_abs)) * np.log(fft_abs / np.sum(fft_abs) + 1e-10))\n",
    "\n",
    "    # Function to calculate spectral centroid\n",
    "    def spectral_centroid(x):\n",
    "        fft_abs = np.abs(np.fft.fft(x))\n",
    "        return np.sum(np.arange(len(x)) * fft_abs) / np.sum(fft_abs)\n",
    "\n",
    "    # Function to calculate spectral spread\n",
    "    def spectral_spread(x):\n",
    "        fft_abs = np.abs(np.fft.fft(x))\n",
    "        centroid = spectral_centroid(x)\n",
    "        return np.sqrt(np.sum(((np.arange(len(x)) - centroid)**2) * fft_abs) / np.sum(fft_abs))\n",
    "\n",
    "    # Function to calculate spectral flatness\n",
    "    def spectral_flatness(x):\n",
    "        fft_abs = np.abs(np.fft.fft(x))\n",
    "        return np.exp(np.mean(np.log(fft_abs + 1e-10))) / np.mean(fft_abs)\n",
    "\n",
    "    # Apply each function separately to the specified column using rolling window\n",
    "    df[f'{column}_fft_energy'] = df[column].rolling(window=window_size, center=True).apply(fft_energy, raw=True)\n",
    "    df[f'{column}_fft_entropy'] = df[column].rolling(window=window_size, center=True).apply(fft_entropy, raw=True)\n",
    "    df[f'{column}_spectral_centroid'] = df[column].rolling(window=window_size, center=True).apply(spectral_centroid, raw=True)\n",
    "    df[f'{column}_spectral_spread'] = df[column].rolling(window=window_size, center=True).apply(spectral_spread, raw=True)\n",
    "    df[f'{column}_spectral_flatness'] = df[column].rolling(window=window_size, center=True).apply(spectral_flatness, raw=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fd517a6-ad7c-41ef-a74d-957cf1b4320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_features(df, window_size=60):\n",
    "    # Initialize an empty DataFrame to store the final result\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each unique 'id' in the input DataFrame 'df'\n",
    "    for unique_id in df['id'].unique():\n",
    "        # Filter the DataFrame for the current unique 'id'\n",
    "        id_df = df[df['id'] == unique_id]\n",
    "        \n",
    "        # Create a temporary DataFrame and make a copy of the filtered DataFrame\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df = id_df.copy()\n",
    "        \n",
    "        # Calculate accelerometer and gyroscope magnitudes\n",
    "        id_df['acc_magnitude'] = np.sqrt(id_df['x']**2 + id_df['y']**2 + id_df['z']**2)\n",
    "        id_df['gyr_magnitude'] = np.sqrt(id_df['x_g']**2 + id_df['y_g']**2 + id_df['z_g']**2)\n",
    "        \n",
    "        # Calculate rolling window statistics for accelerometer data ('x', 'y', 'z')\n",
    "        for axis in ['x', 'y', 'z']:\n",
    "            temp_df[f'acc_{axis}_mean'] = id_df[axis].rolling(window=window_size).mean()\n",
    "            temp_df[f'acc_{axis}_std'] = id_df[axis].rolling(window=window_size).std()\n",
    "            temp_df[f'acc_{axis}_max'] = id_df[axis].rolling(window=window_size).max()\n",
    "            temp_df[f'acc_{axis}_min'] = id_df[axis].rolling(window=window_size).min()\n",
    "            temp_df[f'acc_{axis}_min_max_diff'] = temp_df[f'acc_{axis}_max'] - temp_df[f'acc_{axis}_min']\n",
    "            temp_df[f'acc_{axis}_skew'] = id_df[axis].rolling(window=window_size).skew()\n",
    "            temp_df[f'acc_{axis}_kurtosis'] = id_df[axis].rolling(window=window_size).kurt()\n",
    "            temp_df[f'acc_{axis}_energy'] = id_df[axis].rolling(window=window_size).apply(lambda x: np.sum(x**2), raw=True)\n",
    "            temp_df[f'acc_{axis}_median'] = id_df[axis].rolling(window=window_size).median()\n",
    "            temp_df[f'acc_{axis}_variance'] = id_df[axis].rolling(window=window_size).var()\n",
    "            temp_df[f'acc_{axis}_sum'] = id_df[axis].rolling(window=window_size).sum()\n",
    "        \n",
    "        # Calculate rolling window statistics for gyroscope data ('x_g', 'y_g', 'z_g')\n",
    "        for axis in ['x_g', 'y_g', 'z_g']:\n",
    "            temp_df[f'gyr_{axis}_mean'] = id_df[axis].rolling(window=window_size).mean()\n",
    "            temp_df[f'gyr_{axis}_std'] = id_df[axis].rolling(window=window_size).std()\n",
    "            temp_df[f'gyr_{axis}_max'] = id_df[axis].rolling(window=window_size).max()\n",
    "            temp_df[f'gyr_{axis}_min'] = id_df[axis].rolling(window=window_size).min()\n",
    "            temp_df[f'gyr_{axis}_min_max_diff'] = temp_df[f'gyr_{axis}_max'] - temp_df[f'gyr_{axis}_min']\n",
    "            temp_df[f'gyr_{axis}_skew'] = id_df[axis].rolling(window=window_size).skew()\n",
    "            temp_df[f'gyr_{axis}_kurtosis'] = id_df[axis].rolling(window=window_size).kurt()\n",
    "            temp_df[f'gyr_{axis}_energy'] = id_df[axis].rolling(window=window_size).apply(lambda x: np.sum(x**2), raw=True)\n",
    "            temp_df[f'gyr_{axis}_median'] = id_df[axis].rolling(window=window_size).median()\n",
    "            temp_df[f'gyr_{axis}_variance'] = id_df[axis].rolling(window=window_size).var()\n",
    "            temp_df[f'gyr_{axis}_sum'] = id_df[axis].rolling(window=window_size).sum()    \n",
    "        \n",
    "        # Drop rows with NaN values resulting from rolling window calculations\n",
    "        temp_df.dropna(inplace=True)    \n",
    "        \n",
    "        # Calculate FFT features for each axis ('x', 'y', 'z', 'x_g', 'y_g', 'z_g')\n",
    "        for axis in ['x', 'y', 'z', 'x_g', 'y_g', 'z_g']:\n",
    "            temp_df = add_fft_features(temp_df, axis, window_size)\n",
    "        \n",
    "        # Concatenate the temporary DataFrame 'temp_df' to the result DataFrame 'result_df'\n",
    "        result_df = pd.concat([result_df, temp_df], ignore_index=True)\n",
    "        \n",
    "        # Drop rows with NaN values from the result DataFrame 'result_df'\n",
    "        result_df.dropna(inplace=True)\n",
    "    \n",
    "    # Return the final DataFrame containing all rolling window features and FFT-based features\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ba18124-4723-4744-ad96-bae66d86708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rolling window feature extraction to train_sessions DataFrame\n",
    "rolled_train = rolling_window_features(train_sessions)\n",
    "\n",
    "# Apply rolling window feature extraction to test_sessions DataFrame\n",
    "rolled_test = rolling_window_features(test_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "34225a48-5e18-44f1-a444-7b4ce0796c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232286, 105)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, you can check the shape of the rolled_train DataFrame\n",
    "rolled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64e66656-fd89-477f-863d-327c05c41594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    122001\n",
       "1.0    100220\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate value counts of the 'target' column\n",
    "rolled_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10bd5536-3c3b-4822-ae8d-33e6ef8f31cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    122001\n",
       "1.0    100220\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolled_train['target'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c767dac1-29a4-4f93-9a1a-a6a27d9221ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample class 0 to balance the classes in the training data\n",
    "class_0 = rolled_train[rolled_train['target'] == 0]\n",
    "class_1 = rolled_train[rolled_train['target'] == 1]\n",
    "\n",
    "# Determine the number of samples to keep from class 0 (e.g., same as class 1 or a desired ratio)\n",
    "desired_class_0_samples = len(class_1)\n",
    "downsampled_class_0 = resample(class_0, n_samples=desired_class_0_samples, random_state=42)\n",
    "\n",
    "# Combine the downsampled class 0 with class 1 to create the balanced training dataset\n",
    "balanced_train_sessions = pd.concat([downsampled_class_0, class_1])\n",
    "\n",
    "# Extract features and labels for training\n",
    "X_train = balanced_train_sessions.drop(['id', 't', 'target'], axis=1)\n",
    "y_train = balanced_train_sessions['target']\n",
    "\n",
    "# Extract features and labels for testing (assuming you have already loaded and preprocessed test data)\n",
    "X_test = rolled_test.drop(['id', 't', 'target'], axis=1)\n",
    "y_test = rolled_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46f79099-bcb4-4d7c-a43e-7421b786b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17151  5652]\n",
      " [ 8966 16355]]\n",
      "Random Forest score is: 0.696243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.75      0.70     22803\n",
      "         1.0       0.74      0.65      0.69     25321\n",
      "\n",
      "    accuracy                           0.70     48124\n",
      "   macro avg       0.70      0.70      0.70     48124\n",
      "weighted avg       0.70      0.70      0.70     48124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=19, n_jobs=-1)\n",
    "\n",
    "# Fit the Random Forest model on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix to evaluate the accuracy of the classification\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(cnf_matrix)\n",
    "\n",
    "# Print the accuracy score of the Random Forest model\n",
    "print(\"Random Forest score is: %f\" % rf.score(X_test, y_test))\n",
    "\n",
    "# Print the classification report for detailed precision, recall, F1-score, and support metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
